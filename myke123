# ==============================================================================
# 0. SETUP DE LIBRER√çAS
# ==============================================================================
if (!require("pacman")) install.packages("pacman")

pacman::p_load(
  # Gesti√≥n de datos y Archivos
  tidyverse,      # dplyr, ggplot2, purrr, etc.
  janitor,        # Limpieza de nombres (clean_names)
  readxl,         # Leer Excel
  
  # Modelado y Machine Learning (Tidymodels)
  tidymodels,     # Framework principal
  bonsai,         # CONECTOR CLAVE para LightGBM
  lightgbm,       # El motor del modelo Maestro
  
  # √Årboles y Reglas
  rpart,          # √Årbol simple (Alumno)
  rpart.plot,     # Dibujar el √°rbol tradicional
  
  # Visualizaci√≥n e Interpretaci√≥n
  vip,            # Importancia de variables
  ggdist,         # Raincloud Plots (La lluvia + nube)
  patchwork,      # Unir varios gr√°ficos en uno solo
  isotree         # Isolation Forest (Detecci√≥n de Anomal√≠as r√°pida)
)

# OPCIONAL: Para el gr√°fico de cortes 2D (Si no lo tienes, descomenta para instalar)
# remotes::install_github("grantmcdermott/parttree")
library(parttree) 

# Configuraci√≥n de tema gr√°fico
theme_set(theme_minimal())

### pre pro
# ==============================================================================
# FASE 0.5: CARGA Y PROCESAMIENTO INICIAL (Feature Engineering)
# ==============================================================================

# 1. CARGA DE DATOS CRUDA
# ------------------------------------------------------------------------------
# Leemos el archivo y limpiamos los nombres de columnas autom√°ticamente
# df_import <- read_excel("tu_ruta_del_journal.xlsx") %>% 
#   clean_names() # Convierte "Monto Transaccion" a "monto_transaccion"

# (Simulaci√≥n de datos para que el c√≥digo corra ahora mismo)
df_import <- tibble(
  monto = c(150, 500, 20, 1000),
  score_riesgo = factor(c("0.05", "0.95", "15", "88")), # Viene como Factor sucio
  reservado_alfa_2 = c("PLIN_001", "APPLE_99", "GOOG_55", "POS_00"), # Tu columna clave
  acf_bin = c("414165", "515120", "455188", "414165"),
  fraude_target = c("No", "Yes", "No", "Yes")
)

# 2. SELECCI√ìN DE VARIABLES (EL "INTERRUPTOR")
# ------------------------------------------------------------------------------
# OPCI√ìN A: Selecci√≥n Manual (Descomenta esta l√≠nea para usarla)
# variables_a_usar <- c("monto", "score_riesgo", "reservado_alfa_2", "acf_bin", "fraude_target")

# OPCI√ìN B: Usar TODO el Journal (Por defecto)
# La funci√≥n everything() selecciona todas las columnas disponibles
variables_a_usar <- everything() 

# Aplicamos la selecci√≥n
df_seleccion <- df_import %>%
  select(all_of(variables_a_usar))

# 3. INGENIER√çA DE VARIABLES (TRANSFORMACIONES)
# ------------------------------------------------------------------------------
df_procesado <- df_seleccion %>%
  mutate(
    
    # A. CORRECCI√ìN DE TIPOS DE DATOS
    # ---------------------------------------------------------
    # El Score suele venir como texto/factor. Lo forzamos a n√∫mero.
    score_riesgo = as.numeric(as.character(score_riesgo)),
    
    # El BIN debe ser siempre Texto (Factor) para que no lo sume ni reste
    acf_bin = as.character(acf_bin),
    
    # B. CREACI√ìN DE LA VARIABLE "TIPO_BILLETERA"
    # ---------------------------------------------------------
    # L√≥gica: Analizar los primeros d√≠gitos de 'reservado_alfa_2' (o la columna que tengas)
    tipo_billetera = case_when(
      # Ejemplo: Si empieza con 'APPLE' o ciertos c√≥digos -> Apple Pay
      str_detect(reservado_alfa_2, "^APPLE") | str_sub(reservado_alfa_2, 1, 5) %in% c("12345", "99999") ~ "Apple Pay",
      
      # Ejemplo: Si empieza con 'GOOG' -> Google Pay
      str_detect(reservado_alfa_2, "^GOOG") ~ "Google Pay",
      
      # Ejemplo: Plin
      str_detect(reservado_alfa_2, "PLIN") ~ "Plin",
      
      # Caso por defecto: Si no es nada de lo anterior
      TRUE ~ "Tarjeta Fisica / Otro"
    )
    
    # C. SEGMENTACI√ìN POR BIN (IDEA FUTURA)
    # ---------------------------------------------------------
    # Aqu√≠ podr√≠amos cruzar con una tabla maestra de BINs para saber si es Classic/Gold/Black.
    # segmento_tarjeta = case_when(
    #   acf_bin %in% c("414165", "455120") ~ "Clasica",
    #   acf_bin %in% c("555555") ~ "Black",
    #   TRUE ~ "Desconocido"
    # )
  )

# 4. INSPECCI√ìN R√ÅPIDA (QUALITY CHECK)
# ------------------------------------------------------------------------------
# glimpse() es mejor que str(). Te muestra la variable, el tipo y los primeros datos.
print("--- ESTRUCTURA DE DATOS PROCESADA ---")
glimpse(df_procesado)

# Verificamos si se crearon bien las billeteras
print("--- CONTEO POR BILLETERA ---")
df_procesado %>% count(tipo_billetera)


#### eda
# ==============================================================================
# FASE 1: EDA (EXPLORATORIO CON DATA ORIGINAL)
# ==============================================================================

# 1. Cargar y Limpiar
# ------------------------------------------------------------------------------
# df_raw <- read_excel("ruta_de_tu_archivo.xlsx") 
# Simulamos data para que el script corra ahora mismo
set.seed(123)
df_raw <- tibble(
  monto = c(rnorm(400, 50, 20), rnorm(50, 500, 100)), # Normales vs Fraude
  score_riesgo = c(rnorm(400, 20, 10), rnorm(50, 85, 10)),
  token = sample(c("Fisico", "ApplePay", "GooglePay"), 450, replace = TRUE, prob = c(0.7, 0.2, 0.1)),
  fraude_target = c(rep("No", 400), rep("Yes", 50)) # 50 fraudes
) %>%
  mutate(fraude_target = as.factor(fraude_target))

# 2. El Gr√°fico "Raincloud" (Lluvia + Nube + Caja) - EL MEJOR PARA FRAUDE
# ------------------------------------------------------------------------------
# Muestra: D√≥nde se concentra la mayor√≠a (Nube) y cada caso individual (Lluvia)
plot_rain <- df_raw %>%
  ggplot(aes(x = fraude_target, y = monto, fill = fraude_target)) +
  # La Nube (Densidad)
  stat_halfeye(adjust = 0.5, justification = -0.2, .width = 0, point_colour = NA) +
  # La Caja (Boxplot)
  geom_boxplot(width = 0.12, outlier.color = NA, alpha = 0.5) +
  # La Lluvia (Puntos)
  stat_dots(side = "left", justification = 1.1, binwidth = 10, dotsize = 0.8) +
  scale_fill_manual(values = c("No" = "#1f77b4", "Yes" = "#d62728")) +
  coord_flip() +
  labs(title = "An√°lisis de Montos: Raincloud Plot", 
       subtitle = "Distribuci√≥n real de montos por tipo de transacci√≥n")

print(plot_rain)

# 3. Cruce Categ√≥rico (Barras de Infecci√≥n)
# ------------------------------------------------------------------------------
plot_cat <- df_raw %>%
  count(token, fraude_target) %>%
  group_by(token) %>%
  mutate(pct = n / sum(n)) %>%
  filter(fraude_target == "Yes") %>%
  ggplot(aes(x = reorder(token, pct), y = pct, fill = token)) +
  geom_col() +
  geom_text(aes(label = scales::percent(pct, accuracy = 0.1)), hjust = -0.2) +
  coord_flip() +
  labs(title = "% de Fraude por Billetera/Token", y = "Tasa de Fraude") +
  theme(legend.position = "none")

print(plot_cat)



üëΩ Fase 2: No Supervisado (Miner√≠a + Extracci√≥n de Reglas)
Aqu√≠ aplicamos tu idea genial: Cluster -> Regla. Usamos Isolation Forest para encontrar "cosas raras". A las m√°s raras las etiquetamos como "Cluster An√≥malo" y le pedimos a un √°rbol que nos explique por qu√©.
# ==============================================================================
# FASE 2: NO SUPERVISADO (ANOMAL√çAS -> REGLAS)
# ==============================================================================

# 1. Preparar data num√©rica para el algoritmo (Isolation Forest solo come n√∫meros)
# ------------------------------------------------------------------------------
df_unsupervised <- df_raw %>%
  select(monto, score_riesgo) # Agrega aqu√≠ m√°s variables num√©ricas (Hora, etc.)

# 2. Entrenar Isolation Forest
# ------------------------------------------------------------------------------
iso_model <- isolation.forest(
  df_unsupervised, 
  ndim = 1,      # Cortes simples
  ntrees = 100   # N√∫mero de √°rboles aleatorios
)

# 3. Predecir Anomal√≠a (Score de 0 a 1)
# ------------------------------------------------------------------------------
# Cerca de 1 = Muy an√≥malo, Cerca de 0.5 = Normal
scores_anomalia <- predict(iso_model, df_unsupervised)

# 4. Crear el "Target Artificial" (Cluster de Anomal√≠as)
# ------------------------------------------------------------------------------
# Definimos: El Top 5% m√°s raro es nuestro "Cluster de Inter√©s"
umbral_top_5 <- quantile(scores_anomalia, 0.95)

df_con_cluster <- df_raw %>%
  mutate(
    score_anomalo = scores_anomalia,
    # Aqu√≠ est√° la magia: Convertimos el hallazgo matem√°tico en una etiqueta (1/0)
    es_atipico = ifelse(score_anomalo >= umbral_top_5, "Si", "No")
  )

# 5. EL SURROGATE TREE (Extraer la regla del Cluster)
# ------------------------------------------------------------------------------
# Entrenamos un √°rbol para que nos explique el campo "es_atipico"
arbol_explicativo <- rpart(
  es_atipico ~ monto + score_riesgo + token, # Usamos variables de negocio
  data = df_con_cluster,
  method = "class", # Clasificaci√≥n (Si/No)
  control = rpart.control(cp = 0.01, maxdepth = 3)
)

print("--- REGLA DE LA ANOMAL√çA DETECTADA ---")
rpart.rules(arbol_explicativo, cover = TRUE)

# Visualizar la regla del cluster
rpart.plot(arbol_explicativo, main = "Reglas que explican el Cluster An√≥malo")



ü§ñ Fase 3: Supervisado (Maestro LightGBM + Alumno √Årbol)
Aqu√≠ integramos todo. Usamos LightGBM para detectar el fraude confirmado y el gr√°fico 2D que pediste.

# ==============================================================================
# FASE 3: SUPERVISADO (MAESTRO LIGHTGBM + VISUALIZACI√ìN 2D)
# ==============================================================================

# 1. Preparar Receta
# ------------------------------------------------------------------------------
set.seed(123)
split <- initial_split(df_raw, strata = fraude_target, prop = 0.8)
train_data <- training(split)

receta_maestro <- recipe(fraude_target ~ ., data = train_data) %>%
  step_dummy(all_nominal_predictors()) 

# 2. El Maestro LightGBM (Usando bonsai)
# ------------------------------------------------------------------------------
spec_lgbm <- boost_tree(
  trees = 500,
  tree_depth = 4,
  min_n = 5,       # Poca data, bajamos el n
  learn_rate = 0.05
) %>%
  set_engine("lightgbm") %>% # Aqu√≠ bonsai hace su magia por detr√°s
  set_mode("classification")

wf_maestro <- workflow() %>%
  add_recipe(receta_maestro) %>%
  add_model(spec_lgbm)

fit_maestro <- fit(wf_maestro, data = train_data)

# 3. El Alumno (√Årbol de Reglas) - Para Monitor Plus
# ------------------------------------------------------------------------------
# (Aqu√≠ ir√≠a el c√≥digo que ya hicimos antes de rpart regresi√≥n sobre las probs)
# ... [C√≥digo de destilaci√≥n previo] ...

# 4. EL GR√ÅFICO DE PARTICIONES 2D (RECTANGULAR CUTS) üü©‚¨õ
# ==============================================================================
# Este es el gr√°fico que viste en el video. Muestra c√≥mo el √°rbol "recorta" el mapa.

# Entrenamos un √°rbol simple solo con 2 variables para poder dibujarlo en 2D
arbol_visual <- rpart(
  fraude_target ~ score_riesgo + monto, 
  data = train_data,
  method = "class"
)

# Opci√≥n A: Usando el paquete 'parttree' (Si lograste instalarlo)
# Muestra las regiones rectangulares de decisi√≥n
grafico_cortes <- ggplot(train_data, aes(x = score_riesgo, y = monto)) +
  geom_parttree(data = arbol_visual, alpha = 0.3, aes(fill = fraude_target)) + # Las cajas
  geom_point(aes(col = fraude_target), alpha = 0.7) + # Los puntos reales
  scale_fill_manual(values = c("No" = "#abdda4", "Yes" = "#fdae61")) +
  scale_color_manual(values = c("No" = "#2b83ba", "Yes" = "#d7191c")) +
  labs(title = "Mapa de Decisiones del √Årbol (2D)",
       subtitle = "C√≥mo el algoritmo 'recorta' el espacio Monyo vs Score")

print(grafico_cortes)

# NOTA: Si 'parttree' falla (porque es de GitHub), rpart.plot es tu respaldo,
# pero parttree es el que te da la visualizaci√≥n de "mapa de zonas".
