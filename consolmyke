Perfecto. Vamos a cerrar esto con un pipeline completo, robusto y seguro, usando las dos estrategias seguras que definimos:

üîê Si hay timestamp completo ‚Üí incremental por > last_timestamp

üîê Si solo hay fecha ‚Üí ventana de seguridad (reprocesar √∫ltimo d√≠a)

Sin recalificaciones todav√≠a.
Solo carga diaria robusta + modelo estrella + SQLite persistente.

üß± ARQUITECTURA FINAL (Operativa para ma√±ana)
Access / Excel
      ‚Üì
Python ETL incremental seguro
      ‚Üì
SQLite (fraude_dw.sqlite)
      ‚Üì
Power BI

üîπ 1Ô∏è‚É£ CONFIGURACI√ìN GLOBAL
import pandas as pd
import pyodbc
import sqlite3
from datetime import datetime
from pathlib import Path

DB_PATH = "fraude_dw.sqlite"

STANDARD_COLUMNS = [
    "fecha",
    "monto",
    "comercio",
    "bin",
    "entry_mode",
    "herramienta"
]

üîπ 2Ô∏è‚É£ CONFIGURACI√ìN POR HERRAMIENTA

‚ö† Siempre especificar formato de fecha y fecha m√≠nima.

TOOL_CONFIG = {
    "FRM": {
        "table": "tabla_frm",
        "column_map": {
            "fecha": "fecha_operacion",
            "monto": "monto_dolar_original",
            "comercio": "nombre_comercio",
            "bin": "bin_tarjeta",
            "entry_mode": "acf_entry_mode"
        },
        "fecha_format": "%d/%m/%Y",
        "min_date": "2025-01-01",
        "has_timestamp": False
    },

    "BRM": {
        "table": "tabla_brm",
        "column_map": {
            "fecha": "fecha_brm",
            "monto": "monto_dolares",
            "comercio": "merchant_name",
            "bin": "bin_code",
            "entry_mode": "entry_mode"
        },
        "fecha_format": "%Y-%m-%d",
        "min_date": "2025-01-01",
        "has_timestamp": True
    }
}

üîπ 3Ô∏è‚É£ FUNCI√ìN PARA OBTENER √öLTIMA FECHA CARGADA
def get_last_timestamp(tool_name):

    conn = sqlite3.connect(DB_PATH)

    query = f"""
        SELECT MAX(fecha)
        FROM fact_rechazos
        WHERE herramienta = '{tool_name}'
    """

    result = conn.execute(query).fetchone()[0]
    conn.close()

    return result

üîπ 4Ô∏è‚É£ LECTURA ACCESS INCREMENTAL SEGURA
def read_access_incremental(tool_name, access_path):

    config = TOOL_CONFIG[tool_name]
    table = config["table"]
    fecha_col = config["column_map"]["fecha"]
    has_timestamp = config["has_timestamp"]

    conn_str = (
        r"Driver={Microsoft Access Driver (*.mdb, *.accdb)};"
        rf"DBQ={access_path};"
    )

    conn = pyodbc.connect(conn_str)

    last_timestamp = get_last_timestamp(tool_name)

    if last_timestamp is None:
        # Primera carga
        query = f"SELECT * FROM {table}"
    else:
        if has_timestamp:
            # Incremental exacto por timestamp
            query = f"""
                SELECT *
                FROM {table}
                WHERE {fecha_col} > #{last_timestamp}#
            """
        else:
            # Ventana de seguridad (reprocesar √∫ltimo d√≠a)
            safe_date = pd.to_datetime(last_timestamp) - pd.Timedelta(days=1)
            safe_date_str = safe_date.strftime("%Y-%m-%d")

            query = f"""
                SELECT *
                FROM {table}
                WHERE {fecha_col} >= #{safe_date_str}#
            """

    df = pd.read_sql(query, conn)
    conn.close()

    return df

üîπ 5Ô∏è‚É£ HOMOLOGACI√ìN ROBUSTA
def standardize(df, tool_name):

    config = TOOL_CONFIG[tool_name]
    col_map = config["column_map"]
    fecha_format = config["fecha_format"]
    min_date = pd.to_datetime(config["min_date"])

    rename_dict = {
        original: new
        for new, original in col_map.items()
        if original in df.columns
    }

    df_std = df.rename(columns=rename_dict)

    for col in STANDARD_COLUMNS:
        if col not in df_std.columns:
            df_std[col] = None

    df_std = (
        df_std
        .assign(
            herramienta=tool_name,
            fecha=lambda x: pd.to_datetime(
                x["fecha"],
                format=fecha_format,
                errors="coerce"
            ),
            monto=lambda x: pd.to_numeric(x["monto"], errors="coerce"),
            comercio=lambda x: x["comercio"].astype("string"),
            bin=lambda x: x["bin"].astype("string"),
            entry_mode=lambda x: x["entry_mode"]
                .fillna("DESCONOCIDO")
                .astype("string")
        )
        .query("fecha >= @min_date")
        [STANDARD_COLUMNS]
    )

    assert df_std["fecha"].notna().all(), \
        f"Error parsing fecha {tool_name}"

    return df_std

üîπ 6Ô∏è‚É£ INSERTAR EN SQLITE (CONTROL POR HERRAMIENTA)
def insert_into_sqlite(df, tool_name):

    conn = sqlite3.connect(DB_PATH)

    # Si usamos ventana de seguridad, borrar ese rango primero
    if not TOOL_CONFIG[tool_name]["has_timestamp"]:
        last_timestamp = get_last_timestamp(tool_name)

        if last_timestamp:
            safe_date = pd.to_datetime(last_timestamp) - pd.Timedelta(days=1)
            safe_date_str = safe_date.strftime("%Y-%m-%d")

            conn.execute(f"""
                DELETE FROM fact_rechazos
                WHERE herramienta = '{tool_name}'
                AND fecha >= '{safe_date_str}'
            """)

    df.to_sql(
        "fact_rechazos",
        conn,
        if_exists="append",
        index=False
    )

    conn.commit()
    conn.close()

üîπ 7Ô∏è‚É£ MODELO ESTRELLA (LIGERO PARA POWER BI)
def create_dimensions():

    conn = sqlite3.connect(DB_PATH)

    df = pd.read_sql("SELECT * FROM fact_rechazos", conn)

    dim_fecha = (
        df[["fecha"]]
        .drop_duplicates()
        .reset_index(drop=True)
        .assign(fecha_key=lambda x: x.index + 1)
    )

    dim_herramienta = (
        df[["herramienta"]]
        .drop_duplicates()
        .reset_index(drop=True)
        .assign(herramienta_key=lambda x: x.index + 1)
    )

    dim_comercio = (
        df[["comercio"]]
        .drop_duplicates()
        .reset_index(drop=True)
        .assign(comercio_key=lambda x: x.index + 1)
    )

    dim_bin = (
        df[["bin"]]
        .drop_duplicates()
        .reset_index(drop=True)
        .assign(bin_key=lambda x: x.index + 1)
    )

    conn.close()

    return dim_fecha, dim_herramienta, dim_comercio, dim_bin

üîπ 8Ô∏è‚É£ PIPELINE COMPLETO
def run_pipeline():

    for tool_name in TOOL_CONFIG.keys():

        df_raw = read_access_incremental(
            tool_name,
            f"data/{tool_name.lower()}.accdb"
        )

        if df_raw.empty:
            continue

        df_std = standardize(df_raw, tool_name)

        insert_into_sqlite(df_std, tool_name)

    print("Carga incremental completada.")
