ğŸ”µ 1ï¸âƒ£ Concepto clave

LightGBM directo trabaja con:

lgb.Dataset()
lgb.train()


Y requiere:

Matriz numÃ©rica

Vector label (0/1)

Nada mÃ¡s.

ğŸ”¥ 2ï¸âƒ£ Pipeline completo para tu caso (fraude aprobado)

Voy a asumir que:

Tu target binario ya estÃ¡ creado

Solo trabajas con aprobadas

Ya eliminaste D y P

Ya eliminaste PII e IDs

ğŸ§© PASO 1 â€” Preparar target binario
df_model <- df_base %>%
  filter(!indicador_fraude %in% c("D","P")) %>%
  mutate(
    target = ifelse(indicador_fraude == "F", 1, 0)
  )

ğŸ§© PASO 2 â€” Eliminar columnas que no deben modelarse

Ejemplo:

df_model <- df_model %>%
  select(-indicador_fraude,
         -trx_id,
         -cliente_id,
         -nombre,
         -apellido)


(ajusta segÃºn tu base)

ğŸ§© PASO 3 â€” Convertir categÃ³ricas

LightGBM acepta categÃ³ricas si las pasas como factores.

Pero internamente convertirÃ¡ todo a matriz.

Entonces hacemos:

df_model <- df_model %>%
  mutate(across(where(is.character), as.factor))

ğŸ§© PASO 4 â€” Crear matriz X y vector y
y <- df_model$target
X <- df_model %>% select(-target)

# Convertir factores a numeric (entero)
X <- X %>%
  mutate(across(where(is.factor), as.integer))

X_matrix <- as.matrix(X)

ğŸ”µ 3ï¸âƒ£ Crear dataset LightGBM
library(lightgbm)

dtrain <- lgb.Dataset(
  data = X_matrix,
  label = y
)

ğŸ”µ 4ï¸âƒ£ Definir parÃ¡metros
params <- list(
  objective = "binary",
  metric = "auc",
  learning_rate = 0.05,
  num_leaves = 31,
  max_depth = -1,
  min_data_in_leaf = 50,
  feature_fraction = 0.8,
  bagging_fraction = 0.8,
  bagging_freq = 5,
  verbose = -1
)

ğŸ”µ 5ï¸âƒ£ Entrenar modelo
modelo_lgb <- lgb.train(
  params = params,
  data = dtrain,
  nrounds = 500,
  early_stopping_rounds = 50,
  verbose = 1
)

ğŸ”µ 6ï¸âƒ£ Predicciones
pred <- predict(modelo_lgb, X_matrix)


Eso te devuelve probabilidad de fraude.

ğŸ”µ 7ï¸âƒ£ Evaluar AUC
library(pROC)

auc(y, pred)

ğŸ”¥ 8ï¸âƒ£ Extraer importancia de variables
importance <- lgb.importance(modelo_lgb)
print(importance)

lgb.plot.importance(importance, top_n = 20)


AquÃ­ es donde empieza la magia real.

ğŸ”¥ 9ï¸âƒ£ Ahora viene lo interesante (destilaciÃ³n)

DespuÃ©s puedes hacer:

df_alumno <- X
df_alumno$score_maestro <- pred


Y entrenar tu Ã¡rbol simple con rpart:

library(rpart)

alumno <- rpart(
  score_maestro ~ .,
  data = df_alumno,
  method = "anova",
  control = rpart.control(maxdepth = 4, minsplit = 100)
)


Y extraer reglas:

rpart.plot::rpart.plot(alumno)
